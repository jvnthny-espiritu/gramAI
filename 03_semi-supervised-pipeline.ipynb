{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "sys.path.append(\"src\")\n",
    "\n",
    "from semi_supervised_mlp import LatentMLPClassifier, load_latents_and_labels\n",
    "from train_latent_classifier import train_latent_classifier, evaluate_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_path = \"latents.npy\"\n",
    "labels_path = \"cluster_labels.npy\"\n",
    "save_path = \"mlp_classifier.pth\"\n",
    "log_path = \"mlp_training_log.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: torch.Size([5304, 16384]), Val: torch.Size([1138, 16384]), Test: torch.Size([1137, 16384])\n",
      "Epoch 1/30, Train Loss: 0.02093017\n",
      "   🔎 Val Loss: 0.00000000\n",
      "Epoch 2/30, Train Loss: 0.00000523\n",
      "   🔎 Val Loss: 0.00000000\n",
      "Epoch 3/30, Train Loss: 0.00000096\n",
      "   🔎 Val Loss: 0.00000000\n",
      "Epoch 4/30, Train Loss: 0.00000005\n",
      "   🔎 Val Loss: 0.00000000\n",
      "Epoch 5/30, Train Loss: 0.00000082\n",
      "   🔎 Val Loss: 0.00000000\n",
      "Epoch 6/30, Train Loss: 0.00000030\n",
      "   🔎 Val Loss: 0.00000000\n",
      "Epoch 7/30, Train Loss: 0.00000019\n",
      "   🔎 Val Loss: 0.00000000\n",
      "Epoch 8/30, Train Loss: 0.00000023\n",
      "   🔎 Val Loss: 0.00000000\n",
      "Epoch 9/30, Train Loss: 0.00000007\n",
      "   🔎 Val Loss: 0.00000000\n",
      "Epoch 10/30, Train Loss: 0.00000093\n",
      "   🔎 Val Loss: 0.00000000\n",
      "Epoch 11/30, Train Loss: 0.00000078\n",
      "   🔎 Val Loss: 0.00000000\n",
      "Epoch 12/30, Train Loss: 0.00000138\n",
      "   🔎 Val Loss: 0.00000000\n",
      "Epoch 13/30, Train Loss: 0.00000003\n",
      "   🔎 Val Loss: 0.00000000\n",
      "Epoch 14/30, Train Loss: 0.00000031\n",
      "   🔎 Val Loss: 0.00000000\n",
      "Epoch 15/30, Train Loss: 0.00000496\n",
      "   🔎 Val Loss: 0.00000000\n",
      "⏹️ Early stopping at epoch 15. Best val loss: 0.000000000\n",
      "✅ Model saved to mlp_classifier.pth | Log: mlp_training_log.json | Loss plot: mlp_loss_curve.png\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL semi_supervised_mlp.LatentMLPClassifier was not an allowed global by default. Please use `torch.serialization.add_safe_globals([LatentMLPClassifier])` or the `torch.serialization.safe_globals([LatentMLPClassifier])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnpicklingError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     18\u001b[39m train_latent_classifier(\n\u001b[32m     19\u001b[39m     latents_path=latents_path,\n\u001b[32m     20\u001b[39m     labels_path=labels_path,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     lr=\u001b[32m1e-3\u001b[39m\n\u001b[32m     27\u001b[39m )\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Step 4: Evaluate on test set\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmlp_classifier.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gramAI/src/train_latent_classifier.py:128\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model_path, X_test, y_test)\u001b[39m\n\u001b[32m    126\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    127\u001b[39m model = LatentMLPClassifier(input_dim=X_test.shape[\u001b[32m1\u001b[39m]).to(device)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m model.load_state_dict(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    129\u001b[39m model.eval()\n\u001b[32m    131\u001b[39m X_test, y_test = X_test.to(device), y_test.to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/gramAI/.venv/lib/python3.13/site-packages/torch/serialization.py:1470\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1462\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1463\u001b[39m                     opened_zipfile,\n\u001b[32m   1464\u001b[39m                     map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1467\u001b[39m                     **pickle_load_args,\n\u001b[32m   1468\u001b[39m                 )\n\u001b[32m   1469\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1470\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1471\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1472\u001b[39m             opened_zipfile,\n\u001b[32m   1473\u001b[39m             map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1476\u001b[39m             **pickle_load_args,\n\u001b[32m   1477\u001b[39m         )\n\u001b[32m   1478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[31mUnpicklingError\u001b[39m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL semi_supervised_mlp.LatentMLPClassifier was not an allowed global by default. Please use `torch.serialization.add_safe_globals([LatentMLPClassifier])` or the `torch.serialization.safe_globals([LatentMLPClassifier])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "# Load full dataset\n",
    "latents, labels = load_latents_and_labels(latents_path, labels_path)\n",
    "\n",
    "# Step 1: Split into Train+Val and Test (85/15)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(latents, labels, test_size=0.15, random_state=42, stratify=labels)\n",
    "\n",
    "# Step 2: Split Train+Val into Train and Val (approx 70/15 overall)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1765, random_state=42, stratify=y_temp)\n",
    "# 0.1765 ≈ 15% / 85% to get 15% overall val\n",
    "\n",
    "# Optional: Print shapes\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# Step 3: Train the model\n",
    "train_latent_classifier(\n",
    "    latents_path=latents_path,\n",
    "    labels_path=labels_path,\n",
    "    val_data=(X_val, y_val),\n",
    "    save_path=\"mlp_classifier.pth\",\n",
    "    log_path=\"mlp_training_log.json\",\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    lr=1e-3\n",
    ")\n",
    "\n",
    "# Step 4: Evaluate on test set\n",
    "evaluate_model(\"mlp_classifier.pth\", X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(log_path, 'r') as f:\n",
    "    log = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(log[\"loss_curve\"], label=\"Loss\")\n",
    "plt.axhline(log[\"best_loss\"], color='red', linestyle='--', label=f\"Best Loss: {log['best_loss']:.4f}\")\n",
    "plt.title(\"Training Loss (MLP on Latents)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "latents, labels = load_latents_and_labels(latents_path, labels_path)\n",
    "\n",
    "model = LatentMLPClassifier(input_dim=latents.shape[1])\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = model(latents.to(device)).cpu().numpy()\n",
    "    preds_bin = (preds >= 0.5).astype(int)\n",
    "    y_true = labels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"✅ Evaluation Metrics:\")\n",
    "print(f\"🔢 Accuracy: {accuracy_score(y_true, preds_bin):.4f}\")\n",
    "print(\"\\n📊 Classification Report:\")\n",
    "print(classification_report(y_true, preds_bin))\n",
    "print(\"🧮 Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, preds_bin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
